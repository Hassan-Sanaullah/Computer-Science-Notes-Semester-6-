# Image Segmentation

Let $R$ represent the entire spatial region occupied by an image. Image segmentation is a process that partitions $R$ into $n$ subregions $R_1, R_2, ..., R_n$ such that:

- (a) $R = \bigcup_{i=1}^{n} R_i$  
- (b) $R_i$ is a connected set for $i = 1, 2, ..., n$  
- (c) $R_i \cap R_j = \emptyset$ for all $i \neq j$  
- (d) $Q(R_i) = \text{TRUE}$ for $i = 1, 2, ..., n$  
- (e) $Q(R_i) \neq Q(R_j)$ for any adjacent regions $R_i$ and $R_j$

Where:
- $Q(R_k)$ is a logical predicate defined over the points in set $R_k$, and $\emptyset$ is the null set.
- $\bigcup$ and $\cap$ represent set union and intersection, respectively.
- Two regions $R_i$ and $R_j$ are adjacent if their union forms a connected set.

### Conditions for Segmentation:
1. **Complete Partition**: Every pixel must belong to a region (Condition a).
2. **Connectivity**: Points in a region must be connected (Condition b).
3. **Disjointness**: Regions must not overlap (Condition c).
4. **Predicate Validity**: All pixels in a region must satisfy a certain condition, e.g., having the same intensity (Condition d).
5. **Adjacency**: Adjacent regions must differ in terms of the defined predicate (Condition e).

### Segmentation Approaches
Segmentation algorithms for monochrome images are generally based on two categories:
1. **Discontinuity-based segmentation**: Assumes boundaries of regions are sufficiently different in intensity to allow boundary detection based on local intensity discontinuities (e.g., edge-based segmentation).
2. **Similarity-based segmentation**: Partitions an image into regions that are similar based on predefined criteria.

### Example of Discontinuity-based Segmentation:
- **Figure 10.1(a)** shows an image with a region of constant intensity superimposed on a darker background.
- **Figure 10.1(b)** illustrates the boundary computation based on intensity discontinuities.
- **Figure 10.1(c)** shows the result of segmentation, where pixels on or inside the boundary are labeled white, and all exterior pixels are labeled black.

### Example of Region-based Segmentation:
- **Figure 10.1(d)** shows a textured region with the same constant background as in Figure 10.1(a).
- **Figure 10.1(e)** demonstrates intensity discontinuities, which are difficult to use for boundary detection due to numerous spurious intensity changes.
- **Figure 10.1(f)** shows the result of segmenting the image into 8x8 subregions, labeling each subregion white if the standard deviation of its pixels is non-zero (indicating texture)

# Point, Line, and Edge Detection
## Image Derivatives and Edge Detection

#### 1. Introduction to Derivatives in Image Processing
- **Local Averaging**: Smoothes an image.
- **Derivatives**: Useful to detect abrupt, local intensity changes.

#### 2. Derivative Properties
- **First-Order Derivative**:
  - Sensitive to local changes in intensity.
  - Zero in areas of constant intensity.
  - Nonzero at intensity step or ramp onset.
  
- **Second-Order Derivative**:
  - Zero in constant intensity areas.
  - Nonzero at the onset and end of an intensity step or ramp.
  - Zero along intensity ramps.

#### 3. Digital Derivatives (Finite Differences)
- Approximation of derivatives using finite differences.

##### First-Order Derivative Approximation
- **Forward Difference** (Eq. 10-4):
  $$
  \frac{\partial}{\partial x} f(x) = f(x+1) - f(x)
  $$
- **Backward Difference** (Eq. 10-5):
  $$
  \frac{\partial}{\partial x} f(x) = f(x) - f(x-1)
  $$
- **Central Difference** (Eq. 10-6):
  $$
  \frac{\partial}{\partial x} f(x) = \frac{f(x+1) - f(x-1)}{2}
  $$

##### Second-Order Derivative Approximation
- **Central Difference** (Eq. 10-7):
  $$
  \frac{\partial^2}{\partial x^2} f(x) = f(x+1) - 2f(x) + f(x-1)
  $$

#### 4. Higher-Order Derivatives
- **Third-Order** (Eq. 10-8):
  $$
  \frac{\partial^3}{\partial x^3} f(x) = f(x+2) - 3f(x+1) + 3f(x-1) - f(x-2)
  $$
- **Fourth-Order** (Eq. 10-9):
  $$
  \frac{\partial^4}{\partial x^4} f(x) = f(x+2) - 4f(x+1) + 6f(x) - 4f(x-1) + f(x-2)
  $$

#### 5. Symmetry and Central Difference
- **Symmetry** of central differences results in lower error for the same number of points.
- For two variables:
  $$
  \frac{\partial^2}{\partial x^2} f(x,y) = f(x+1, y) - 2f(x, y) + f(x-1, y)
  $$
  $$
  \frac{\partial^2}{\partial y^2} f(x,y) = f(x, y+1) - 2f(x, y) + f(x, y-1)
  $$

#### 6. Edge Detection and Derivatives
- **First-Order Derivatives**:
  - Produce "thicker" edges, useful for detecting larger changes in intensity.
- **Second-Order Derivatives**:
  - Detect fine detail, such as thin lines, noise, and isolated points.
  - Produce "double-edge" effect at step or ramp transitions.
  - Sign of second derivative can determine light-to-dark or dark-to-light transitions.

#### 7. Practical Computation: Convolution
- **Spatial Convolution**: 
  - Used to apply derivatives at every pixel in an image.
  - For a 3x3 filter, the response at a pixel is computed by summing the products of the filter coefficients and the intensity values of the surrounding pixels.
  
  Formula:
  $$
  Z_k = \sum_{k=1}^{9} w_k \cdot z_k
  $$
  where $Z_k$ is the intensity of the pixel at the $k$-th location, and $w_k$ are the filter coefficients.


# Detection of Isolated Points

- **Second Derivative and Laplacian**:  
  Point detection should be based on the second derivative, which is represented by the Laplacian operator. This operator in two dimensions is given by:
  
  $$
  \nabla^2 f(x, y) = \frac{\partial^2 f(x, y)}{\partial x^2} + \frac{\partial^2 f(x, y)}{\partial y^2} \quad \text{(Equation 10-13)}
  $$

  This equation is derived from the expansion of Eq. (3-35) for a $3 \times 3$ kernel, and uses simplified subscript notation for kernel coefficients.

- **Finite Differences for Derivatives**:  
  The partial derivatives in the Laplacian can be computed using second-order finite differences, as shown in the following form:

  $$
  \nabla^2 f(x, y) = f(x, y) + f(x-1, y) + f(x+1, y) + f(x, y-1) + f(x, y+1) - 4f(x, y) \quad \text{(Equation 10-14)}
  $$

- **Laplacian Kernel**:  
  The Laplacian can be implemented using a kernel as shown in Figure 10.4(a) in the example. This kernel computes the differences between the center pixel and its 8 neighbors, highlighting isolated points.

- **Point Detection**:  
  A point is detected at location $(x, y)$ if the absolute value of the response of the Laplacian kernel at that point exceeds a certain threshold. The formula for detection is:

  $$
  g(x, y) = 
  \begin{cases}
    1 & \text{if } |Z(x, y)| > T \\
    0 & \text{otherwise}
  \end{cases} \quad \text{(Equation 10-15)}
  $$

  Where:
  - $g(x, y)$ is the output binary image (1 for detected points, 0 for others).
  - $T$ is a nonnegative threshold.
  - $Z(x, y)$ is the result of the Laplacian kernel applied at $(x, y)$.
  
  The kernel response measures the intensity differences between the pixel and its neighbors. An isolated point, which has an intensity that differs significantly from its surrounding pixels, will be detected when these differences exceed the threshold.

- **Properties of Laplacian Kernel**:  
  The coefficients of the Laplacian kernel sum to zero, indicating that the filter response will be zero in areas of constant intensity (i.e., homogeneous regions).

- **Example of Isolated Point Detection**:  
  - **Image**: An X-ray image of a turbine blade from a jet engine (Fig. 10.4(b)) has a porosity, manifesting as a single black pixel in the upper-right quadrant.
  - **Filtered Image**: After applying the Laplacian kernel (Fig. 10.4(c)), the isolated point is clearly detected.
  - **Thresholding**: The result of Eq. (10-15) with $T$ equal to 90% of the highest absolute pixel value of the filtered image is shown in Fig. 10.4(d). The isolated point at the tip of the arrow is visible.
  
  This method works best when the isolated point is surrounded by a homogeneous background. If the surrounding area is not homogeneous, other methods for detecting intensity changes may be more suitable.

# Line Detection

- **Laplacian for Line Detection**:  
  The second derivative, as represented by the Laplacian, can be used for line detection. It is expected to give a stronger response and produce thinner lines compared to first derivatives. However, the **double-line effect** of the second derivative needs to be handled properly.

- **Example of Line Detection with the Laplacian**:  
  - **Original Image**: Figure 10.5(a) shows a portion of a wire-bond mask from an electronic circuit.
  - **Laplacian Image**: Figure 10.5(b) is the result of applying the Laplacian kernel, showing both positive and negative values.
  - **Absolute Value of Laplacian**: Figure 10.5(c) shows the absolute value of the Laplacian image, which results in doubled line thickness.
  - **Positive Values of Laplacian**: Figure 10.5(d) shows using only the positive values of the Laplacian, which helps in achieving thinner lines. Thresholding is used to eliminate random noise fluctuations.

- **Double-Line Effect**:  
  The double-line effect is visible in the Laplacian results, especially when the lines are wide compared to the kernel size. When using a $3 \times 3$ kernel, lines that are 5 pixels wide result in zero response at the center. Lines not satisfying the assumption of being thin relative to the detector are better handled by edge detection methods.

- **Isotropic Laplacian Kernel**:  
  The Laplacian kernel is **isotropic**, meaning it responds equally to edges in all directions (vertical, horizontal, and diagonal).

- **Directional Line Detection**:  
  When detecting lines in specific orientations, kernels can be designed to respond maximally to certain directions. The kernels in Figure 10.6 are designed to detect lines in four directions:
  - Kernel 1: Horizontal lines (0°)
  - Kernel 2: Diagonal lines (+45°)
  - Kernel 3: Vertical lines (90°)
  - Kernel 4: Diagonal lines (-45°)

  **Response to Directions**: The maximum response is obtained when a line is oriented according to the kernel. If the response from one kernel ($Z_k$) is greater than the responses from all other kernels ($Z_j$), then the point is more likely associated with a line in the direction of kernel $k$.

- **Thresholding for Line Detection**:  
  After filtering the image with these directional kernels, the absolute value of the response is thresholded. The non-zero points left after thresholding correspond to lines in the detected direction.

- **Example of Directional Line Detection**:  
  - **Image Used**: Figure 10.7(a) shows an image from the previous example.
  - **Detecting +45° Lines**: Using the kernel from Figure 10.6(b), lines oriented at +45° are detected.
  - **Filtered Result**: The result in Figure 10.7(b) shows the filtered image, with darker shades indicating negative values.
  - **Zoomed Sections**: Figures 10.7(c) and (d) show zoomed sections of the filtered image, revealing two main line segments oriented at +45°.
  - **Strength of Response**: The line segment in Figure 10.7(d) is brighter because it is one pixel thick, whereas the segment in Figure 10.7(c) is thicker.
  - **Thresholding**: In Figure 10.7(e), the positive values of the filtered image are shown, and a threshold $T = 254$ is applied to detect strong responses.
  - **Final Detection**: Figure 10.7(f) shows the points that exceed the threshold, representing the detected lines in the image.

- **Handling Isolated Points**:  
  In the final image, isolated points can be deleted using morphological operators or by using the Laplacian kernel (Fig. 10.4(a)) and then removing points with weak responses.

# Edge Models in Image Segmentation

Edge detection is a technique frequently used for segmenting images based on abrupt (local) changes in intensity. This section covers various ways to model edges and approaches for detecting them.

## Types of Edge Models

### 1. **Step Edge**
   - A **step edge** is characterized by a transition between two intensity levels, ideally occurring over the distance of one pixel.
   - Example: Used in computer-generated images for solid modeling and animation.
   - The **Canny edge detection algorithm** was derived using a step-edge model.

### 2. **Ramp Edge**
   - **Ramp edges** model blurred edges, with the slope of the ramp inversely proportional to the degree of blurring.
   - In practice, edges are often blurred and noisy, with noise mainly due to electronic components in imaging systems.
   - A ramp edge has no single "edge point" but consists of a segment of points connected along the ramp.

### 3. **Roof Edge**
   - **Roof edges** represent lines through regions, where the base (width) of the edge depends on the thickness and sharpness of the line.
   - Example: Found in range imaging when thin objects are closer to a sensor, such as pipes in a range image.

### 4. **Combination of Edges**
   - Real-world images may contain all three types of edges (step, ramp, and roof).
   - Despite noise and blurring, edges in reasonably sharp images resemble these ideal edge models.

## Intensity Profiles and Derivatives

### Intensity Profile Analysis
- **Step Edge**: Sharp transition between two intensity levels.
- **Ramp Edge**: Gradual transition with a defined slope.
- **Roof Edge**: Characterized by a width that represents the thickness of the edge.

### First and Second Derivatives
- **First Derivative**: Measures the rate of change in intensity. It is used to detect the presence of edges.
- **Second Derivative**: Measures the change in the rate of intensity change. Zero crossings in the second derivative are often used to locate edges in an image.
- **Zero Crossing**: Marks a point of transition between the positive and negative phases of the second derivative.

### Example of Intensity Profile and Derivatives (Fig. 10.10)
- **First Derivative**: Positive at the onset of the ramp and at points on the ramp, zero at points of constant intensity.
- **Second Derivative**: Positive at the beginning of the ramp, negative at the end, zero at points on the ramp.

### Conclusion from Derivatives
- **Magnitude of the first derivative**: Can be used to detect the presence of an edge.
- **Sign of the second derivative**: Helps determine whether an edge pixel lies on the dark or light side of the edge.
- **Zero crossing of the second derivative**: Useful for detecting thick edges' centers.

## Noise Sensitivity of Derivatives

### Example 10.4: Behavior of Derivatives with Noise
- **Noise Impact**: Gaussian noise affects both the first and second derivatives significantly.
- **First Derivative**: Becomes increasingly difficult to associate with a ramp edge as noise increases.
- **Second Derivative**: More sensitive to noise, and its ability to detect edges diminishes with high levels of noise.

### Observations
- Noise can have a significant impact on the effectiveness of edge detection using derivatives, even when the noise is visually undetectable in the image.
- **Image smoothing** should be considered before applying derivatives in edge detection, especially in the presence of noise.

## Edge Detection Process

### Typical Steps in Edge Detection
1. **Image Smoothing**: Reduce noise by smoothing the image before applying edge detection.
2. **Edge Detection**: A local operation that identifies potential edge points in the image.
3. **Edge Localization**: Refines the edge points to select only those that are part of actual edges.

### Example of Noisy Edge Detection (Fig. 10.11)
- Images with various levels of Gaussian noise are processed to show the impact on the first and second derivatives.
- **First Derivative**: Increases in noise cause it to become harder to distinguish from the original ramp edge.
- **Second Derivative**: More heavily affected by noise, making edge detection harder as noise increases.

# Basic Edge Detection

## The Image Gradient and Its Properties

- **Gradient**: The gradient is the tool of choice for finding edge strength and direction at an arbitrary location \((x, y)\) of an image \(f\).
  - The gradient vector \(\nabla f\) is defined as:
    $$
    \nabla f(x, y) = \begin{bmatrix} \frac{\partial f(x, y)}{\partial x} \\ \frac{\partial f(x, y)}{\partial y} \end{bmatrix}
    $$
  - The gradient points in the direction of maximum rate of change of the intensity \(f\).

- **Magnitude of the Gradient**: The magnitude of the gradient vector at a point \((x, y)\) is given by its Euclidean vector norm:
  $$
  M(x, y) = \sqrt{\left(\frac{\partial f(x, y)}{\partial x}\right)^2 + \left(\frac{\partial f(x, y)}{\partial y}\right)^2}
  $$
  - \(M(x, y)\) gives the rate of change in the direction of the gradient at point \((x, y)\).

- **Gradient Direction**: The direction of the gradient vector at a point \((x, y)\) is given by:
  $$
  \alpha(x, y) = \tan^{-1}\left(\frac{\frac{\partial f(x, y)}{\partial y}}{\frac{\partial f(x, y)}{\partial x}}\right)
  $$
  - Angles are measured counterclockwise with respect to the \(x\)-axis.

### Example 10.5: Computing the Gradient

- **Example Setup**: 
  - Consider a zoomed section of an image containing a straight edge segment, where the shaded pixels are assumed to have a value of 0 and the white pixels have a value of 1.

- **Step-by-Step Calculation**:
  1. Use a \(3 \times 3\) neighborhood centered at a point.
  2. To compute the gradient in the x-direction (\(\frac{\partial f}{\partial x}\)):
     - Subtract the pixels in the top row from the pixels in the bottom row.
  3. To compute the gradient in the y-direction (\(\frac{\partial f}{\partial y}\)):
     - Subtract the pixels in the left column from the pixels in the right column.
  4. Using the differences from these steps, compute the partial derivatives:
     - \(\frac{\partial f}{\partial x} = -2\) and \(\frac{\partial f}{\partial y} = 2\).
  5. The gradient vector becomes:
     $$
     \nabla f = \begin{bmatrix} -2 \\ 2 \end{bmatrix}
     $$
  6. The magnitude of the gradient is:
     $$
     f = \sqrt{2^2 + 2^2} = 2\sqrt{2}
     $$
  7. The direction of the gradient vector is:
     $$
     \alpha = \tan^{-1}\left(\frac{2}{-2}\right) = -45^\circ
     $$
     - This is the gradient direction, and the edge direction is orthogonal to this, at \(90^\circ - (-45^\circ) = 135^\circ\).

### Key Concepts

- **Gradient Vector**: The gradient points in the direction of the greatest rate of change of intensity.
- **Edge Direction**: The direction of the edge at a point is orthogonal to the gradient vector at that point.
- **Edge Normal**: The gradient vector is sometimes referred to as the edge normal. If it is normalized to unit length, it becomes the edge unit normal.


# Gradient Operators

Obtaining the gradient of an image involves computing the partial derivatives $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ at every pixel location.

### Forward and Centered Finite Difference
Using forward differences, we obtain:

- **Gradient in x-direction**:
  $$
  g_x(x, y) = \frac{\partial f(x, y)}{\partial x} = f(x+1, y) - f(x, y)
  $$

- **Gradient in y-direction**:
  $$
  g_y(x, y) = \frac{\partial f(x, y)}{\partial y} = f(x, y+1) - f(x, y)
  $$

These equations can be implemented for all values of $x$ and $y$ by filtering $f(x, y)$ with 1-D kernels.

### Diagonal Edge Detection
When diagonal edge direction is of interest, 2-D kernels are required. The Roberts cross-gradient operators, developed by Roberts (1965), are one of the earliest attempts to use 2-D kernels with a diagonal preference.

- **Gradient in x-direction**:
  $$
  g_x = \frac{\partial f}{\partial x} = f(9) - f(5)
  $$

- **Gradient in y-direction**:
  $$
  g_y = \frac{\partial f}{\partial y} = f(8) - f(6)
  $$

These derivatives can be implemented by filtering an image with specific kernels.

### Prewitt Operators
Prewitt operators (Prewitt [1970]) are an improvement over Roberts' operators. They use kernels of size $3 \times 3$ for better accuracy in approximating the derivatives.

- **Gradient in x-direction**:
  $$
  g_x = f(7) + f(8) + f(9) - f(1) - f(2) - f(3)
  $$

- **Gradient in y-direction**:
  $$
  g_y = f(3) + f(6) + f(9) - f(1) - f(4) - f(7)
  $$

These kernels provide more accurate edge detection than Roberts' operators by considering the data on opposite sides of the center point.

### Sobel Operators
A variation of the Prewitt operator uses a center weight of 2 for image smoothing:

- **Gradient in x-direction**:
  $$
  g_x = 2f(7) + 2f(8) + 2f(9) - f(1) - f(2) - f(3)
  $$

- **Gradient in y-direction**:
  $$
  g_y = 2f(3) + 2f(6) + 2f(9) - f(1) - f(4) - f(7)
  $$

The Sobel operators provide better noise suppression than Prewitt operators, which is useful in real-world applications where noise can be an issue.

### Gradient Magnitude and Direction
The gradient magnitude is computed as:

$$
M(x, y) = \sqrt{g_x^2(x, y) + g_y^2(x, y)}
$$

However, a computationally simpler approximation uses absolute values:

$$
M(x, y) \approx |g_x(x, y)| + |g_y(x, y)|
$$

This approximation maintains relative changes in intensity levels but is not isotropic for all edge directions.

### Kirsch Compass Kernels
The Kirsch compass kernels are designed to detect edges in all eight compass directions, allowing more directional specificity in edge detection.

- Instead of calculating magnitude and angle separately, Kirsch's method involves convolving the image with all eight kernels and assigning the edge magnitude as the response of the kernel that gives the strongest value at each point.
- The edge direction is then determined by the direction associated with the strongest kernel response.

### Example: Sobel Gradient Magnitude and Direction
Figures show the Sobel absolute value response of the two gradient components $g_x$ and $g_y$, as well as the gradient image formed from the sum of these components. The directionality of the horizontal and vertical components is evident in the images.

- **Gradient angle image**:
  The gradient angle image can be computed using:
  $$
  \theta(x, y) = \text{atan2}(g_y(x, y), g_x(x, y))
  $$
  This provides information on edge orientation, useful for edge detection algorithms like Canny.

### Image Smoothing
Fine details, such as small bricks, may cause noise and complicate edge detection. To reduce this, the image can be smoothed using a filter like a $5 \times 5$ averaging filter before computing gradients.

### Response of Kirsch Kernels
- The Kirsch kernels allow for better differentiation of diagonal edges (e.g., $45^\circ$ and $-45^\circ$) compared to Sobel or Prewitt operators.
- This stronger diagonal selectivity is evident when applying the $45^\circ$ (NW) and $-45^\circ$ (SW) Kirsch kernels.

---

**Note**: The gradients, edge magnitude, and direction provide critical information for edge detection and segmentation, and kernel selection (e.g., Sobel, Prewitt, Kirsch) depends on the application and desired edge directionality.


# Combining the Gradient with Thresholding

The following concepts describe how to combine gradient and thresholding for better edge detection results.

## Edge Detection Enhancement

- **Smoothing the Image**: Prior to computing the gradient, smoothing the image helps in reducing noise and fine details that could interfere with edge detection. This approach can make the edge detection more selective.

- **Thresholding the Gradient Image**: After computing the gradient image, thresholding can be applied to make edge detection more focused by eliminating small edges and emphasizing more significant edges.

### Example:

- **Fig. 10.20(a)**: The gradient image (from Fig. 10.16(d)) is thresholded so that:
  - Pixels with gradient values greater than or equal to 33% of the maximum gradient value are shown in white.
  - Pixels below the threshold are shown in black.
- **Result**: 
  - Fewer edges are visible.
  - The edges that remain are sharper (e.g., edges in the roof tile).
  - Some edges, such as the sloping line of the roof, are broken in the thresholded image.

### Combining Smoothing and Thresholding:

- **Objective**: Highlight principal edges and maintain as much connectivity as possible.
- **Fig. 10.20(b)**: Shows the result of thresholding after smoothing the image (Fig. 10.18(d)).
  - This method reduces the number of broken edges compared to just thresholding the unsmoothed gradient (Fig. 10.20(a)).
  - For example, edges like those indicated by arrows in the previous figures appear more continuous.

## Summary

- Thresholding and smoothing work together to make edge detection more robust by removing smaller, irrelevant edges and enhancing larger, more significant features.
- Thresholding sharpens edges but may break continuity, while smoothing reduces fine detail and noise, leading to better connectivity in edge detection results.
